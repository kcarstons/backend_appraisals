{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f47ac9-bf7e-4f8a-903f-24b4f93017f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 20:43:21.387618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "import pypdfium2 as pdfium \n",
    "#from IPython.display import display, HTML #only need for domino\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import bitsandbytes\n",
    "\n",
    "# goal\n",
    "# 1) pick 2 or 3 language models that do image annotations \n",
    "# 2) run them on small sample of images from appraisals\n",
    "# 3) manually look at annotations and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ad56e9-a17d-48ee-8883-ff910ff44044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_s3(file_name):\n",
    "    \"\"\"\n",
    "    Read one file in at a time.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = 'a1156-val'\n",
    "    data_key = f\"backend_appraisals/sample_data/{file_name}.pdf\"\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key) \n",
    "\n",
    "    pdf_file = s3.get_object(Bucket = bucket, Key = data_key)[\n",
    "        \"Body\"\n",
    "    ].read()\n",
    "\n",
    "    return pdf_file\n",
    "\n",
    "\n",
    "def extract_images(file_name): \n",
    "    \"\"\"\n",
    "    Extract all images from a pdf and store in a list. \n",
    "    \"\"\"\n",
    "    all_images = []\n",
    "    pdf = pdfium.PdfDocument(BytesIO(read_from_s3(file_name)))\n",
    "\n",
    "    for i in range(len(pdf)):\n",
    "        page = pdf[i]\n",
    "        \n",
    "        for obj in page.get_objects(): \n",
    "            if obj.type == 3: \n",
    "                image = obj.get_bitmap().to_pil() \n",
    "                all_images.append(image)\n",
    "\n",
    "    return all_images\n",
    "        \n",
    "    \n",
    "# def extract_images_and_nearby_text(file_name): \n",
    "#     \"\"\"\n",
    "#     Extracts images and nearby text from each page and returns a list of dictionaries \n",
    "#     with each image and nearby text (if applicable). \n",
    "#     \"\"\"\n",
    "#     pdf = pdfium.PdfDocument(BytesIO(read_from_s3(file_name)))\n",
    "#     images_and_text_all_pages = []\n",
    "\n",
    "#     #loop through every page in pdf\n",
    "#     for i in range(len(pdf)):\n",
    "#         page = pdf[i] \n",
    "#         images_with_positions = [] \n",
    "\n",
    "#         #extract each image and position \n",
    "#         for obj in page.get_objects(): \n",
    "#             if obj.type == 3: \n",
    "#                 image = obj.get_bitmap().to_pil() \n",
    "#                 pos = obj.get_pos() \n",
    "#                 images_with_positions.append((image, pos)) \n",
    "\n",
    "#         if images_with_positions:\n",
    "#             page_text = page.get_textpage()\n",
    "#             #images_and_text = []\n",
    "\n",
    "#             for image, pos in images_with_positions:\n",
    "#                 #expand the image box to the right \n",
    "#                 expanded_box = (pos[0], pos[1] + 150, pos[2] + 250, pos[3])\n",
    "#                 nearby_text = page_text.get_text_bounded(*expanded_box)\n",
    "\n",
    "#                 images_and_text_all_pages.append({\n",
    "#                     'image': image,\n",
    "#                     'label': nearby_text\n",
    "#                 })\n",
    "\n",
    "#             #images_and_text_all_pages.append(images_and_text)\n",
    "\n",
    "#     return images_and_text_all_pages\n",
    "\n",
    "# extracted_images_and_labels = extract_images_and_nearby_text(\"appraisal_sf\")\n",
    "# #list comprehension to filter\n",
    "# subject_images = [x for x in extracted_images_and_labels if \"comparable\" not in x['label'].lower()][3:]\n",
    "\n",
    "\n",
    "\n",
    "def captioner(data_dict, hf_model): \n",
    "    captioner = pipeline(\"image-to-text\", model = hf_model)\n",
    "\n",
    "    for item in data_dict:\n",
    "        pil_image = item['image']\n",
    "        label = item['label']\n",
    "        caption = captioner(pil_image)\n",
    "        print(f\"Label: {label}, Generated Caption: {caption[0]['generated_text']}\")\n",
    "        #pil_image.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46768519-5e38-4b91-8cb2-9db25909763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_images_sf = extract_images(\"appraisal_sf\")\n",
    "#applicable to subject property - keep manual for now including labels since sample doc different format\n",
    "subject_images_sf = extracted_images_sf[3:12]\n",
    "\n",
    "labels_sf = [ 'subject front', 'subject rear', 'subject street', 'kitchen', 'nook', 'living/dining', 'bedroom', 'bathroom', 'bathroom']\n",
    "\n",
    "labeled_images_sf = [{'image':img, 'label': lbl} for img, lbl in zip(subject_images_sf, labels_sf)]\n",
    "\n",
    "# for image in subject_images_sf: \n",
    "#     image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "69e90300-a183-4398-8f1b-850760bdcb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping langugage-image pre-training = BLIP (general image captioning) - trained on clean and noisy web data (vit backbone)\n",
    "# visual transformer is image encoder and text transformer is encoder-decoder\n",
    "\n",
    "# blip_base_output_sf = captioner(labeled_images_sf,  \"Salesforce/blip-image-captioning-large\")\n",
    "# blip_base_output_sf\n",
    "\n",
    "\n",
    "# Label: subject front, Generated Caption: a rendering of a house on a hill\n",
    "# Label: subject rear, Generated Caption: a rendering of a house with a garden and a walkway\n",
    "# Label: subject street, Generated Caption: a rendering of a small house in the middle of a town --> town is interesting lol\n",
    "# Label: kitchen, Generated Caption: a kitchen and dining area in a modern apartment\n",
    "# Label: nook, Generated Caption: a room with a table and chairs in it\n",
    "# Label: living/dining, Generated Caption: a living room with a couch and a table\n",
    "# Label: bedroom, Generated Caption: a room with a couch and a window\n",
    "# Label: bathroom, Generated Caption: a bathroom with a toilet and shelves\n",
    "# Label: bathroom, Generated Caption: a white bed --> wrong"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc8c5c0c-ebdb-4529-b9eb-c9aeafbcb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping langugage-image pre-training = BLIP (general image captioning) - trained on clean and noisy web data \n",
    "# large built with a vit-l backbone --> winner at room type and can tell if it's modern but we need to see about condition\n",
    "\n",
    "# blip_large_output_sf = captioner(labeled_images_sf,  \"Salesforce/blip-image-captioning-large\")\n",
    "# blip_large_output_sf\n",
    "\n",
    "# Label: subject front, Generated Caption: rendering of a modern home with a deck and a covered patio\n",
    "# Label: subject rear, Generated Caption: rendering of a modern home with a garden and walkway\n",
    "# Label: subject street, Generated Caption: rendering of a small house with a balcony and a balcony\n",
    "# Label: kitchen, Generated Caption: there is a kitchen with a table and chairs in it\n",
    "# Label: nook, Generated Caption: there is a table with chairs and a plant in a room\n",
    "# Label: living/dining, Generated Caption: there is a living room with a couch, table, chairs and a television\n",
    "# Label: bedroom, Generated Caption: there is a bed in a room with a lot of windows\n",
    "# Label: bathroom, Generated Caption: there is a white bathroom with a toilet and a shower\n",
    "# Label: bathroom, Generated Caption: there is a white bathroom with a sink and a mirror\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0d298-d044-412c-9442-85379400922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vit gpt2 (multimodal - combined vision transformer and gpt2 for text generation)\n",
    "\n",
    "# vit_gpt2_output_sf = captioner(labeled_images_sf,  \"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# vit_gpt2_output_sf\n",
    "\n",
    "# Label: subject front, Generated Caption: a white and black dog standing in front of a fence --> wrong\n",
    "# Label: subject rear, Generated Caption: a large building with a large window on top \n",
    "# Label: subject street, Generated Caption: a building with a fence and a bench in front of it \n",
    "# Label: kitchen, Generated Caption: a kitchen with a table and a stove \n",
    "# Label: nook, Generated Caption: a kitchen with a table and a plant \n",
    "# Label: living/dining, Generated Caption: a living room with a couch, chairs, and a table \n",
    "# Label: bedroom, Generated Caption: a large room with a couch and a window \n",
    "# Label: bathroom, Generated Caption: a bathroom with a toilet and a shower \n",
    "# Label: bathroom, Generated Caption: a bathroom with a sink, toilet and bathtub \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc20529-d4b1-4a62-9436-e8b1291c3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generativeimage2text(git) from microsoft\n",
    "# base was terrible; large was too large and i think blip large was better anyway\n",
    "\n",
    "# git_output_sf = captioner(labeled_images_sf,  \"microsoft/git-base\")\n",
    "# git_output_sf\n",
    "\n",
    "# Label: subject front, Generated Caption: the building is white\n",
    "# Label: subject rear, Generated Caption: the house in the middle of the street\n",
    "# Label: subject street, Generated Caption: the house is the pink one in the middle\n",
    "# Label: kitchen, Generated Caption: white kitchen cabinets\n",
    "# Label: nook, Generated Caption: the window in the room\n",
    "# Label: living/dining, Generated Caption: a large window\n",
    "# Label: bedroom, Generated Caption: the house is on the market for $ 3. 5 million.\n",
    "# Label: bathroom, Generated Caption: a white shower stall\n",
    "# Label: bathroom, Generated Caption: a mirror on the wall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef9d0fd-3b54-4d3f-8274-d47393d600fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a rendering of a modern house with a deck\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     label \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m     caption \u001b[38;5;241m=\u001b[39m generate_caption(pil_image)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Generated Caption: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcaption\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m#pil_image.close()\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \n\u001b[1;32m     39\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# captioner = pipeline(\"image-to-text\", model = hf_model)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#blip 2 - zero shot image to text gen\n",
    "# pip install accelerate\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\")\n",
    "\n",
    "# def captioner(data_dict, hf_model): \n",
    "#     captioner = pipeline(\"image-to-text\", model = hf_model)\n",
    "\n",
    "#     for item in data_dict:\n",
    "#         pil_image = item['image']\n",
    "#         label = item['label']\n",
    "#         caption = captioner(pil_image)\n",
    "#         print(f\"Label: {label}, Generated Caption: {caption[0]['generated_text']}\")\n",
    "#         #pil_image.close()\n",
    "\n",
    "\n",
    "\n",
    "def generate_caption(pil_image):\n",
    "    inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "\n",
    "    generated_ids = model.generate(**inputs)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(generated_text)\n",
    "\n",
    "\n",
    "for item in labeled_images_sf:\n",
    "    pil_image = item['image']\n",
    "    label = item['label']\n",
    "    caption = generate_caption(pil_image)\n",
    "    \n",
    "    print(f\"Label: {label}, Generated Caption: {caption[0]['generated_text']}\")\n",
    "        #pil_image.close()\n",
    "    \n",
    "\n",
    "\n",
    "# question = \"how many dogs are in the picture?\"\n",
    "# inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# out = blip2_model.generate(**inputs)\n",
    "# print(blip2_processor.decode(out[0], skip_special_tokens=True).strip())\n",
    "\n",
    "\n",
    "# captioner = pipeline(\"image-to-text\", model = hf_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72497c2-58e1-47d2-a0ae-e3e42430640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llava - multimodal object detection image captioner\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "def generate_caption(pil_image): \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "\n",
    "    model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "    # processor = AutoProcessor.from_pretrained(model_id)\n",
    "    # model = LlavaForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n",
    "\n",
    "\n",
    "    pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})\n",
    "\n",
    "\n",
    "    max_new_tokens = 200\n",
    "    prompt = \"USER: <image>\\nPlease describe this image in detail - what room is it and what is the condition?\\nASSISTANT:\"\n",
    "    outputs = pipe(pil_image, prompt = prompt, generate_kwargs={\"max_new_tokens\":200})\n",
    "    #print(outputs[0][\"generated_text\"])\n",
    "\n",
    "\n",
    "for item in labeled_images_sf:\n",
    "    pil_image = item['image']\n",
    "    label = item['label']\n",
    "    caption = generate_caption(pil_image)\n",
    "    print(f\"Label: {label}, Generated Caption: {outputs[0]['generated_text']}\")\n",
    "        #pil_image.close()\n",
    "\n",
    "# git_output_sf = captioner(labeled_images_sf,  \"microsoft/git-base\")\n",
    "# git_output_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e1594-7fe1-4d8b-9a7c-121b278882ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# florence - multimodal object detection image captioner\n",
    "def captioner(data_dict, hf_model): \n",
    "    captioner = pipeline(\"image-to-text\", model = hf_model)\n",
    "\n",
    "    for item in data_dict:\n",
    "        pil_image = item['image']\n",
    "        label = item['label']\n",
    "        caption = captioner(pil_image)\n",
    "        print(f\"Label: {label}, Generated Caption: {caption[0]['generated_text']}\")\n",
    "        #pil_image.close()\n",
    "\n",
    "\n",
    "\n",
    "captioner = pipeline(\"image-to-text\", model = hf_model)\n",
    "\n",
    "# florence_output_sf = captioner(labeled_images_sf,  \"microsoft/git-base\")\n",
    "# florence_output_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192afac-57cf-48de-9af5-8f0367f22bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_images_fha = extract_images(\"fha_appraisal\")\n",
    "#applicable to subject property - keep manual for now including labels since sample doc different format\n",
    "subject_images_fha = extracted_images_fha[11:18]\n",
    "\n",
    "labels_fha = ['Subject Front', 'Subject Rear', 'Subject Street', 'Interior', 'Interior', 'Interior', 'Interior']\n",
    "\n",
    "labeled_images_fha = [{'image':img, 'label': lbl} for img, lbl in zip(subject_images_fha, labels_fha)]\n",
    "\n",
    "for image in subject_images_fha: \n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb19f69-b6fd-4d0c-9be7-410ebf3df741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping langugage-image pre-training = BLIP (general image captioning) - trained on clean and noisy web data \n",
    "# large built with a vit-l backbone --> winner\n",
    "\n",
    "blip_large_output_fha = captioner(labeled_images_fha,  \"Salesforce/blip-image-captioning-large\")\n",
    "blip_large_output_fha\n",
    "\n",
    "# Label: Subject Front, Generated Caption: this is a house with a red door and a red door\n",
    "# Label: Subject Rear, Generated Caption: there is a house that is sitting in the grass\n",
    "# Label: Subject Street, Generated Caption: cars parked on the side of the road in a residential area\n",
    "# Label: Interior, Generated Caption: there is a kitchen with a refrigerator, stove, sink and a window\n",
    "# Label: Interior, Generated Caption: there is a small room with a television and a lamp\n",
    "# Label: Interior, Generated Caption: there is a bed with a green blanket and a green blanket on it\n",
    "# Label: Interior, Generated Caption: there is a bathroom with a toilet, sink, and bathtub\n",
    "\n",
    "\n",
    "#fairly accurate at room type but not great at providing detail / cant pick up on condition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
