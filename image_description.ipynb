{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1869d0-9ad3-4a3f-87b2-3d6e231a46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "import pypdfium2 as pdfium \n",
    "#from IPython.display import display, HTML #only need for domino\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, pipeline, Blip2Processor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "import bitsandbytes\n",
    "\n",
    "# goal\n",
    "# 1) pick 2 or 3 language models that do image annotations \n",
    "# 2) run them on small sample of images from appraisals\n",
    "# 3) manually look at annotations and evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdcf854-7851-4847-8a79-590a571d3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_s3(file_name):\n",
    "    \"\"\"\n",
    "    Read one file in at a time.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = 'a1156-val'\n",
    "    data_key = f\"backend_appraisals/sample_data/{file_name}.pdf\"\n",
    "    data_location = 's3://{}/{}'.format(bucket, data_key) \n",
    "\n",
    "    pdf_file = s3.get_object(Bucket = bucket, Key = data_key)[\n",
    "        \"Body\"\n",
    "    ].read()\n",
    "\n",
    "    return pdf_file\n",
    "\n",
    "\n",
    "def extract_images(file_name): \n",
    "    \"\"\"\n",
    "    Extract all images from a pdf and store in a list. \n",
    "    \"\"\"\n",
    "    all_images = []\n",
    "    pdf = pdfium.PdfDocument(BytesIO(read_from_s3(file_name)))\n",
    "\n",
    "    for i in range(len(pdf)):\n",
    "        page = pdf[i]\n",
    "        \n",
    "        for obj in page.get_objects(): \n",
    "            if obj.type == 3: \n",
    "                image = obj.get_bitmap().to_pil() \n",
    "                all_images.append(image)\n",
    "\n",
    "    return all_images\n",
    "        \n",
    "    \n",
    "# def extract_images_and_nearby_text(file_name): \n",
    "#     \"\"\"\n",
    "#     Extracts images and nearby text from each page and returns a list of dictionaries \n",
    "#     with each image and nearby text (if applicable). \n",
    "#     \"\"\"\n",
    "#     pdf = pdfium.PdfDocument(BytesIO(read_from_s3(file_name)))\n",
    "#     images_and_text_all_pages = []\n",
    "\n",
    "#     #loop through every page in pdf\n",
    "#     for i in range(len(pdf)):\n",
    "#         page = pdf[i] \n",
    "#         images_with_positions = [] \n",
    "\n",
    "#         #extract each image and position \n",
    "#         for obj in page.get_objects(): \n",
    "#             if obj.type == 3: \n",
    "#                 image = obj.get_bitmap().to_pil() \n",
    "#                 pos = obj.get_pos() \n",
    "#                 images_with_positions.append((image, pos)) \n",
    "\n",
    "#         if images_with_positions:\n",
    "#             page_text = page.get_textpage()\n",
    "#             #images_and_text = []\n",
    "\n",
    "#             for image, pos in images_with_positions:\n",
    "#                 #expand the image box to the right \n",
    "#                 expanded_box = (pos[0], pos[1] + 150, pos[2] + 250, pos[3])\n",
    "#                 nearby_text = page_text.get_text_bounded(*expanded_box)\n",
    "\n",
    "#                 images_and_text_all_pages.append({\n",
    "#                     'image': image,\n",
    "#                     'label': nearby_text\n",
    "#                 })\n",
    "\n",
    "#             #images_and_text_all_pages.append(images_and_text)\n",
    "\n",
    "#     return images_and_text_all_pages\n",
    "\n",
    "# extracted_images_and_labels = extract_images_and_nearby_text(\"appraisal_sf\")\n",
    "# #list comprehension to filter\n",
    "# subject_images = [x for x in extracted_images_and_labels if \"comparable\" not in x['label'].lower()][3:]\n",
    "\n",
    "\n",
    "# pipeline captioner\n",
    "def captioner(data_dict, hf_model): \n",
    "    captioner = pipeline(\"image-to-text\", model = hf_model)\n",
    "\n",
    "    for item in data_dict:\n",
    "        pil_image = item['image']\n",
    "        label = item['label']\n",
    "        caption = captioner(pil_image)\n",
    "        print(f\"Label: {label}, Generated Caption: {caption[0]['generated_text']}\")\n",
    "        #pil_image.close()\n",
    "\n",
    "\n",
    "def mm_generate_caption(pil_image):\n",
    "\n",
    "    inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(**inputs)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def answer_prompt(pil_image, prompt):\n",
    "\n",
    "    inputs = processor(images=pil_image, text = prompt, return_tensors=\"pt\")\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens = 20)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda6e3c-6fd9-4af2-8e24-91a0d23290aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_images_sf = extract_images(\"appraisal_sf\")\n",
    "#applicable to subject property - keep manual for now including labels since sample doc different format\n",
    "subject_images_sf = extracted_images_sf[3:12]\n",
    "\n",
    "labels_sf = [ 'subject front', 'subject rear', 'subject street', 'kitchen', 'nook', 'living/dining', 'bedroom', 'bathroom', 'bathroom']\n",
    "\n",
    "labeled_images_sf = [{'image':img, 'label': lbl} for img, lbl in zip(subject_images_sf, labels_sf)]\n",
    "\n",
    "for image in subject_images_sf: \n",
    "    image.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a7c958-e387-4206-ac04-ecdf128be347",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_images_fha = extract_images(\"fha_appraisal\")\n",
    "#applicable to subject property - keep manual for now including labels since sample doc different format\n",
    "subject_images_fha = extracted_images_fha[11:18]\n",
    "\n",
    "labels_fha = ['Subject Front', 'Subject Rear', 'Subject Street', 'Interior', 'Interior', 'Interior', 'Interior']\n",
    "\n",
    "labeled_images_fha = [{'image':img, 'label': lbl} for img, lbl in zip(subject_images_fha, labels_fha)]\n",
    "\n",
    "for image in subject_images_fha: \n",
    "    image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfffcc0-c72a-4192-8a0b-98424bc4750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## BLIP LARGE #####################################################\n",
    "# bootstrapping langugage-image pre-training = BLIP (general image captioning) - trained on clean and noisy web data \n",
    "# large built with a vit-l backbone --> winner at room type and can tell if it's modern but we need to see about condition\n",
    "\n",
    "# NICE HOUSE\n",
    "# blip_large_output_sf = captioner(labeled_images_sf,  \"Salesforce/blip-image-captioning-large\")\n",
    "# blip_large_output_sf\n",
    "\n",
    "# Label: subject front, Generated Caption: rendering of a modern home with a deck and a covered patio\n",
    "# Label: subject rear, Generated Caption: rendering of a modern home with a garden and walkway\n",
    "# Label: subject street, Generated Caption: rendering of a small house with a balcony and a balcony\n",
    "# Label: kitchen, Generated Caption: there is a kitchen with a table and chairs in it\n",
    "# Label: nook, Generated Caption: there is a table with chairs and a plant in a room\n",
    "# Label: living/dining, Generated Caption: there is a living room with a couch, table, chairs and a television\n",
    "# Label: bedroom, Generated Caption: there is a bed in a room with a lot of windows\n",
    "# Label: bathroom, Generated Caption: there is a white bathroom with a toilet and a shower\n",
    "# Label: bathroom, Generated Caption: there is a white bathroom with a sink and a mirror\n",
    "\n",
    "\n",
    "\n",
    "# FHA\n",
    "# large built with a vit-l backbone --> winner\n",
    "\n",
    "# blip_large_output_fha = captioner(labeled_images_fha,  \"Salesforce/blip-image-captioning-large\")\n",
    "# blip_large_output_fha\n",
    "\n",
    "# NOTE: fairly accurate at room type but not great at providing detail / cant pick up on condition\n",
    "# Label: Subject Front, Generated Caption: this is a house with a red door and a red door\n",
    "# Label: Subject Rear, Generated Caption: there is a house that is sitting in the grass\n",
    "# Label: Subject Street, Generated Caption: cars parked on the side of the road in a residential area\n",
    "# Label: Interior, Generated Caption: there is a kitchen with a refrigerator, stove, sink and a window\n",
    "# Label: Interior, Generated Caption: there is a small room with a television and a lamp\n",
    "# Label: Interior, Generated Caption: there is a bed with a green blanket and a green blanket on it\n",
    "# Label: Interior, Generated Caption: there is a bathroom with a toilet, sink, and bathtub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756d10a-4764-40e8-b9f7-0db674138751",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## BLIP2 CAPTION #######################################################\n",
    "# zero shot image to text gen\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\")\n",
    "\n",
    "# NICE HOUSE\n",
    "# for item in labeled_images_sf:\n",
    "#     pil_image = item['image']\n",
    "#     label = item['label']\n",
    "#     caption = mm_generate_caption(pil_image)\n",
    "#     print(f\"Label: {label}, Generated Caption: {caption}\")\n",
    "\n",
    "# NOTES: on par with blip large but cleaner (maybe slightly less detail. should see with a distresed property)\n",
    "# Label: subject front, Generated Caption: a rendering of a modern house with a deck\n",
    "# Label: subject rear, Generated Caption: a rendering of a modern house with a deck\n",
    "# Label: subject street, Generated Caption: a rendering of a modern house with a balcony\n",
    "# Label: kitchen, Generated Caption: a kitchen with a table and chairs in it\n",
    "# Label: nook, Generated Caption: a kitchen and dining area in a modern home\n",
    "# Label: living/dining, Generated Caption: a modern living room with a couch, coffee table and a dining table\n",
    "# Label: bedroom, Generated Caption: a 3d rendering of a bedroom with a bed and a window\n",
    "# Label: bathroom, Generated Caption: a bathroom with a shower, toilet and shelf\n",
    "# Label: bathroom, Generated Caption: a bathroom with a sink, toilet and a potted plant\n",
    "\n",
    "\n",
    "\n",
    "# for item in labeled_images_fha:\n",
    "#     pil_image = item['image']\n",
    "#     label = item['label']\n",
    "#     caption = mm_generate_caption(pil_image)\n",
    "#     print(f\"Label: {label}, Generated Caption: {caption}\")\n",
    "\n",
    "# NOTE: better at the text out (clearer)\n",
    "#Label: Subject Front, Generated Caption: a house with a driveway and trees in the yard\n",
    "# Label: Subject Rear, Generated Caption: a gray house with a large yard and trees\n",
    "# Label: Subject Street, Generated Caption: a street with trees and houses on both sides\n",
    "# Label: Interior, Generated Caption: a kitchen with a refrigerator, stove and sink\n",
    "# Label: Interior, Generated Caption: a room with a lamp, a window and a hardwood floor\n",
    "# Label: Interior, Generated Caption: a bed with a green comforter and a white nightstand\n",
    "# Label: Interior, Generated Caption: a bathroom with a toilet, tub and window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1e092-0b26-4623-9895-343df45bf5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## BLIP2 VQA #####################################################\n",
    "\n",
    "# need to fine-tine blip2 on kaggle home data - it thinks every house is in good condition and nothing is wrong when clearly there is a big difference\n",
    "\n",
    "# for item in labeled_images_sf:\n",
    "#     pil_image = item['image']\n",
    "#     label = item['label']\n",
    "#     response = answer_prompt(pil_image, \"Question: Is there is any wall damage, furniture, mold or other special attributes. Answer:\")\n",
    "#     print(response)\n",
    "\n",
    "\n",
    "\n",
    "# for item in labeled_images_fha:\n",
    "#     pil_image = item['image']\n",
    "#     label = item['label']\n",
    "#     response = answer_prompt(pil_image, \"Question: Is there any furniture in the room? Answer:\")\n",
    "#     print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12392d6-c3ce-478b-a412-f39a331daa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## LLAVA ####################################################\n",
    "# object detection\n",
    "\n",
    "# from transformers import pipeline\n",
    "# from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# import torch\n",
    "\n",
    "# def generate_caption(pil_image): \n",
    "\n",
    "#     model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "#     # processor = AutoProcessor.from_pretrained(model_id)\n",
    "#     # model = LlavaForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n",
    "\n",
    "#     pipe = pipeline(\"image-to-text\", model=model_id)\n",
    "\n",
    "\n",
    "#     max_new_tokens = 200\n",
    "#     prompt = \"USER: <image>\\nPlease describe this image in detail - what room is it and what is the condition?\\nASSISTANT:\"\n",
    "#     outputs = pipe(pil_image, prompt = prompt, generate_kwargs={\"max_new_tokens\":200})\n",
    "#     #print(outputs[0][\"generated_text\"])\n",
    "\n",
    "\n",
    "# for item in labeled_images_sf:\n",
    "#     pil_image = item['image']\n",
    "#     label = item['label']\n",
    "#     caption = generate_caption(pil_image)\n",
    "#     print(f\"Label: {label}, Generated Caption: {outputs[0]['generated_text']}\")\n",
    "#         #pil_image.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a9c57-dac5-4e93-827d-127d355c45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## FLORENCE ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c2fe6-57a5-4b38-9c5c-687ac0f8a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPENDINX\n",
    "\n",
    "###################################  BLIP Base #######################################\n",
    "# bootstrapping langugage-image pre-training = BLIP (general image captioning) - trained on clean and noisy web data (vit backbone)\n",
    "# visual transformer is image encoder and text transformer is encoder-decoder\n",
    "\n",
    "# blip_base_output_sf = captioner(labeled_images_sf,  \"Salesforce/blip-image-captioning-large\")\n",
    "# blip_base_output_sf\n",
    "\n",
    "\n",
    "# Label: subject front, Generated Caption: a rendering of a house on a hill\n",
    "# Label: subject rear, Generated Caption: a rendering of a house with a garden and a walkway\n",
    "# Label: subject street, Generated Caption: a rendering of a small house in the middle of a town --> town is interesting lol\n",
    "# Label: kitchen, Generated Caption: a kitchen and dining area in a modern apartment\n",
    "# Label: nook, Generated Caption: a room with a table and chairs in it\n",
    "# Label: living/dining, Generated Caption: a living room with a couch and a table\n",
    "# Label: bedroom, Generated Caption: a room with a couch and a window\n",
    "# Label: bathroom, Generated Caption: a bathroom with a toilet and shelves\n",
    "# Label: bathroom, Generated Caption: a white bed --> wrong\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################  generativeimage2text(git) from microsoft #############################\n",
    "# base was terrible; large was too large and i think blip large was better anyway\n",
    "\n",
    "# git_output_sf = captioner(labeled_images_sf,  \"microsoft/git-base\")\n",
    "# git_output_sf\n",
    "\n",
    "# Label: subject front, Generated Caption: the building is white\n",
    "# Label: subject rear, Generated Caption: the house in the middle of the street\n",
    "# Label: subject street, Generated Caption: the house is the pink one in the middle\n",
    "# Label: kitchen, Generated Caption: white kitchen cabinets\n",
    "# Label: nook, Generated Caption: the window in the room\n",
    "# Label: living/dining, Generated Caption: a large window\n",
    "# Label: bedroom, Generated Caption: the house is on the market for $ 3. 5 million.\n",
    "# Label: bathroom, Generated Caption: a white shower stall\n",
    "# Label: bathroom, Generated Caption: a mirror on the wall\n",
    "\n",
    "\n",
    "##########################################  VIT GP2 ######################################\n",
    "#vit gpt2 (multimodal - combined vision transformer and gpt2 for text generation)\n",
    "\n",
    "# vit_gpt2_output_sf = captioner(labeled_images_sf,  \"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# vit_gpt2_output_sf\n",
    "\n",
    "\n",
    "#NOTE: This was fine but BLIP was better\n",
    "# Label: subject front, Generated Caption: a white and black dog standing in front of a fence --> wrong\n",
    "# Label: subject rear, Generated Caption: a large building with a large window on top \n",
    "# Label: subject street, Generated Caption: a building with a fence and a bench in front of it \n",
    "# Label: kitchen, Generated Caption: a kitchen with a table and a stove \n",
    "# Label: nook, Generated Caption: a kitchen with a table and a plant \n",
    "# Label: living/dining, Generated Caption: a living room with a couch, chairs, and a table \n",
    "# Label: bedroom, Generated Caption: a large room with a couch and a window \n",
    "# Label: bathroom, Generated Caption: a bathroom with a toilet and a shower \n",
    "# Label: bathroom, Generated Caption: a bathroom with a sink, toilet and bathtub "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
